[
  {
    "objectID": "chapters/3_RAG/rag_classification.html",
    "href": "chapters/3_RAG/rag_classification.html",
    "title": "Chapter 5. RAG Pipeline for Automatic Text Classification",
    "section": "",
    "text": "In this chapter, we will explore an alternative approach to text classification building a simple retrieval-augmented generation (RAG) system. This chapter will cover: * Building a knowledge base using official NACE category descriptions. * Retrieving the top_k relevant categories based on a natural language query. * Using a large language model (LLM) to make the final classification.\nimport pandas as pd\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline",
    "crumbs": [
      "RAG Classification"
    ]
  },
  {
    "objectID": "chapters/3_RAG/rag_classification.html#phase-1---knowledge-base-creation",
    "href": "chapters/3_RAG/rag_classification.html#phase-1---knowledge-base-creation",
    "title": "Chapter 5. RAG Pipeline for Automatic Text Classification",
    "section": "Phase 1 - Knowledge Base Creation",
    "text": "Phase 1 - Knowledge Base Creation\nThe first step in any RAG system is to build a knowledge base. &gt; In the context of retrieval-augmented generation, a knowledge base is a structured collection of external information (e.g., documents, descriptions…) that can be searched and retrieved from. This is usually employed to ground LLM responses into factual information instead on relying solely on the model’s internal knowledge, or to augment the model with knowledge that was not available in the training set.\nIn order to build a knowledge base we need to: 1. Define what documents to store and how to structure them. 2. Embed the selected texts using a sentence-level embedding model (SentenceTransformer). 3. Store the embeddings in memory or in a vector database.\n\nDefining and structuring documents\nIn our case, the knowledge base will be comprised by the NACE codes at the division level (2-digit). Therefore, the first step is to load the file containing all NACE codes and pre-process it.\n\nnace_df = pd.read_excel(\"data/NACE_Rev2.1_Structure_Explanatory_Notes_EN.xlsx\")\n\nNow, we can isolate the NACE divisions.\n\nnace_division_df = nace_df[nace_df[\"ID\"].str.len() == 2].reset_index(drop=True)\n\nnace_division_df.head()\n\nAs we can see, the dataset has multiple textual columns, like titles and descriptions (Includes, etc.). &gt; In general, the more context we give to the embedding model the better, as long as the final text to embed doesn’t get too long.\nA simple way to exploit the textual information present in the NACE dataset, is to include different textual features into a structured template, which we can call descriptor. In this setting, each NACE division will be associated to one descriptor, made of its title and further information present in the dataset.\n\ncodes = nace_division_df[\"ID\"].tolist()\ntitles = nace_division_df[\"NAME\"].tolist()\ndescriptions = nace_division_df[\"Includes\"].tolist()\n\ndescriptor_template = \"{title}.\\n{description}\"\n\ndescriptors = []\nfor title, description in zip(titles, descriptions):\n    descriptors.append(\n        descriptor_template.format(title=title.upper(), description=description)\n    )\n\nLet’s print one descriptor.\n\nprint(descriptors[5])\n\nThese descriptors will be the actual text that will be embedded to create the knowledge base.\n\n\nEmbed the descriptors\nNow that we built a descriptor for each of the 80+ NACE divisions, we need to embed them using a sentence-level embedding model. There are plenty of such models on Hugging Face. We can load them using the SentenceTransformer class from the sentence_transformers library.\nFor this example, we will use the lightweight multilingual paraphrase-multilingual-MiniLM-L12-v2 model from the S-BERT family. Since we need to match descriptions in French with NACE codes in English, it is important that our model can handle multiple languages.\n\nMODEL_ID: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n\nmodel = SentenceTransformer(MODEL_ID)\n\nAfter loading the model, we can embed the descriptors. Each descriptor will be embedded into a 384-dimensional embedding vector.\n\nembeddings = model.encode(descriptors, convert_to_tensor=True)\n\nSince we only have 87 NACE divisions (and therefore 87 vectors), for the purpose of this tutorial we can keep the embeddings in memory without relying on vector databases.",
    "crumbs": [
      "RAG Classification"
    ]
  },
  {
    "objectID": "chapters/3_RAG/rag_classification.html#phase-2---searching-the-knowledge-base",
    "href": "chapters/3_RAG/rag_classification.html#phase-2---searching-the-knowledge-base",
    "title": "Chapter 5. RAG Pipeline for Automatic Text Classification",
    "section": "Phase 2 - Searching the Knowledge Base",
    "text": "Phase 2 - Searching the Knowledge Base\nThis phase consists in developing a method to search the knowledge base via semantic similarity. &gt; When searching a knowledge base, a natural language query is embedded via the same model used to create the knowledge base and a distance metric is computed to evaluate the similarity between the query and the documents in the knowledge base.\nThe most popular distance metric used to retrieve elements from a knowledge base is cosine similarity, defined as: \\[\\cos (\\theta)=\\frac{\\mathbf{A}\\cdot \\mathbf{B}}{||\\mathbf{A}||||\\mathbf{B}||}\\] where \\(\\theta\\) is the angle between the embedding vectors \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\).\nNow we can finally define a function that searches the knowledge base given a natural language query and retrieves the top_k most similar results.\n\ndef search_base(\n    query: str,\n    base: torch.Tensor,\n    embedding_model: SentenceTransformer,\n    top_k: int = 5\n):\n    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n    cos_scores = torch.nn.functional.cosine_similarity(base, query_embedding)\n    top_results = torch.topk(cos_scores, k=top_k)\n\n    return top_results\n\nLet’s test the function with a sample query. We get both values (similarity scores) and indices as a result. In order to extract the corresponding NACE divisions, we can use the indices to filter the NACE dataframe.\n\nsample_query = \"Attività di estrazione mineraria.\"\n\ntop_results = search_base(sample_query, embeddings, model, top_k=5)\n\nfor i, sim in zip(top_results.indices, top_results.values):\n    code = nace_division_df.iloc[int(i)][\"ID\"]\n    title = nace_division_df.iloc[int(i)][\"NAME\"]\n    print(f\"{code}: {title}\\nSimilarity: {sim:.3f}\\n\")\n\nIn theory, we could stop here and just pick the first result as our classification guess. However, a more robust approach involves an LLM judge to pick between the candidates.",
    "crumbs": [
      "RAG Classification"
    ]
  },
  {
    "objectID": "chapters/3_RAG/rag_classification.html#phase-3---final-llm-classification",
    "href": "chapters/3_RAG/rag_classification.html#phase-3---final-llm-classification",
    "title": "Chapter 5. RAG Pipeline for Automatic Text Classification",
    "section": "Phase 3 - Final LLM Classification",
    "text": "Phase 3 - Final LLM Classification\nThis final stage comprises a generative large language model to pick an option between the retrieved candidate codes. First, we load the LLM via the Hugging Face pipeline wrapper.\n\nLLM_ID: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=LLM_ID,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n\nSystem Prompt and Instructions\nWe need to define a system prompt, i.e., textual guidelines that will help the LLM follow our instructions, and the actual prompt template to parse the LLM input.\n\nSYSTEM_PROMPT: str = \"\"\"You are an economic activity classifier.\n\nBased on a description, return ONLY the most fitting candidate without any additional text.\"\"\"\n\nPROMPT_TEMPLATE: str = \"\"\"DESCRIPTION: \"{query}\"\n\nCANDIDATES:\\n{candidates}\"\"\"\n\n\n\nParsing the Input\nWe need to extract the list of candidates using semantic-based similarity and parse it to make it understandable for the LLM.\n\nsample_query = \"Mining activities\"\n\ntop_results = search_base(sample_query, embeddings, model, top_k=5)\n\ncandidates_list = []\nfor i, sim in zip(top_results.indices, top_results.values):\n    code = nace_division_df.iloc[int(i)][\"ID\"]\n    title = nace_division_df.iloc[int(i)][\"NAME\"]\n    candidates_list.append(f\"{code} - {title}\")\n\ncandidates = \"\\n\".join(candidates_list)\n\nLet’s print the parsed prompt to visualize it.\n\nparsed_prompt = PROMPT_TEMPLATE.format(query=sample_query, candidates=candidates)\n\nprint(parsed_prompt)\n\n\n\nFull RAG System\nNow, we can test the full RAG system to classify the economic activity given a natural language query.\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": parsed_prompt},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=32,\n    pad_token_id=pipe.tokenizer.eos_token_id\n)\n\nllm_out = outputs[0][\"generated_text\"][-1][\"content\"]\nprint(llm_out)",
    "crumbs": [
      "RAG Classification"
    ]
  },
  {
    "objectID": "chapters/3_RAG/rag_classification.html#interactive-demo",
    "href": "chapters/3_RAG/rag_classification.html#interactive-demo",
    "title": "Chapter 5. RAG Pipeline for Automatic Text Classification",
    "section": "Interactive Demo",
    "text": "Interactive Demo\nFinally, we can create a small interactive demo for our RAG system. First, we need to create a rag function to perform the whole pipeline.\n\ndef rag(\n    query: str,\n    embeddings: torch.Tensor,\n    embedding_model: SentenceTransformer,\n    llm: pipeline,\n    system_prompt: str,\n    prompt_template: str,\n    nace_df: pd.DataFrame,\n    top_k: int = 5\n):\n\n    top_results = search_base(query, embeddings, embedding_model, top_k=5)\n\n    candidates_list = []\n    for i, sim in zip(top_results.indices, top_results.values):\n        code = nace_df.iloc[int(i)][\"ID\"]\n        title = nace_df.iloc[int(i)][\"NAME\"]\n        candidates_list.append(f\"{code} - {title}\")\n\n    candidates = \"\\n\".join(candidates_list)\n\n    parsed_prompt = PROMPT_TEMPLATE.format(query=query, candidates=candidates)\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": parsed_prompt},\n    ]\n\n    outputs = pipe(\n        messages,\n        max_new_tokens=16,\n        pad_token_id=pipe.tokenizer.eos_token_id\n    )\n\n    return outputs[0][\"generated_text\"][-1][\"content\"]\n\nNow we can run the demo.\n\nquery = input(\"Entry a query ('EXIT' to stop): \")\n\nwhile query != \"EXIT\":\n    output = rag(query, embeddings, model, pipe, SYSTEM_PROMPT, PROMPT_TEMPLATE, nace_division_df)\n    print(f\"RESULT: {output}\")\n    print(\"\")\n    query = input(\"Entry a query ('EXIT' to stop): \")",
    "crumbs": [
      "RAG Classification"
    ]
  },
  {
    "objectID": "chapters/1_DataAugmentation/data_augmentation.html",
    "href": "chapters/1_DataAugmentation/data_augmentation.html",
    "title": "LLM-based synthetic data generation",
    "section": "",
    "text": "The goal of this notebook is to explain how to use LLMs for synthetic data generation, especially in the context of statistical classifications. This is an important step towards training a classifier, since it helps to mitigate some problems with real-world data like scarcity, mislabelling and data gaps.\n#!pip install openai\nimport pandas as pd\nimport openai #for LLMs calls",
    "crumbs": [
      "Data augmentation"
    ]
  },
  {
    "objectID": "chapters/1_DataAugmentation/data_augmentation.html#why-do-we-need-synthetic-data",
    "href": "chapters/1_DataAugmentation/data_augmentation.html#why-do-we-need-synthetic-data",
    "title": "LLM-based synthetic data generation",
    "section": "1. Why do we need synthetic data?",
    "text": "1. Why do we need synthetic data?\nReal-world data for training classifications models have several issues: - Statistical classifications are very complex and have lots of nuances and caveats, so it is not always clear how to classify a given text. - Sometimes data is annotated by non-experts, which lead to mistakes. - Some texts are ambiguous, meaning that there is not enough information to reliably classify them in one class. - Some classes are hugely underrepresented in real-world data (by the structure of the economy), so it can be difficult to obtain enough samples for that data.\nMoreover, obtaining more real-world data is usually a very resource-expensive task. However, nowadays we have at our disposal the generative capabilities of LLMs, which produce text which is nearly indistinguishable from human-produced text. We can exploit this to generate as many real-looking samples as we want very quickly and in a completely automated way.\nEmpirically it has been observed that mixing real-world data with good-quality synthetic data improves substantially the accuracy of the classification models.",
    "crumbs": [
      "Data augmentation"
    ]
  },
  {
    "objectID": "chapters/1_DataAugmentation/data_augmentation.html#reading-the-data",
    "href": "chapters/1_DataAugmentation/data_augmentation.html#reading-the-data",
    "title": "LLM-based synthetic data generation",
    "section": "2. Reading the data",
    "text": "2. Reading the data\n\n2.1 Sample data\nHere we read the real-world sample data available to train the model and do some easy explanatory analysis to check class coverage.\n\ndf = pd.read_parquet(\n    \"https://minio.lab.sspcloud.fr/projet-formation/diffusion/mlops/data/firm_activity_data.parquet\"\n)\ndf.head()\n\n\n\n\n\n\n\n\nnace\ntext\n\n\n\n\n0\n8220Z\nMISSIONS PONCTUELLES A L AIDE D UNE PLATEFORME\n\n\n1\n8553Z\nINSPECTEUR AUTOMOBILE\n\n\n2\n5520Z\nLA LOCATION TOURISTIQUE DE LOGEMENTS INSOLITES...\n\n\n3\n4791A\nCOMMERCE DE TOUT ARTICLES ET PRODUITS MARCHAND...\n\n\n4\n9499Z\nREGROUPEMENT RETRAITE\n\n\n\n\n\n\n\n\nnace_2digit_to_section = {\n    # Section A: Agriculture, forestry and fishing\n    \"01\": \"A\", \"02\": \"A\", \"03\": \"A\",\n\n    # Section B: Mining and quarrying\n    \"05\": \"B\", \"06\": \"B\", \"07\": \"B\", \"08\": \"B\", \"09\": \"B\",\n\n    # Section C: Manufacturing\n    \"10\": \"C\", \"11\": \"C\", \"12\": \"C\", \"13\": \"C\", \"14\": \"C\",\n    \"15\": \"C\", \"16\": \"C\", \"17\": \"C\", \"18\": \"C\", \"19\": \"C\",\n    \"20\": \"C\", \"21\": \"C\", \"22\": \"C\", \"23\": \"C\", \"24\": \"C\",\n    \"25\": \"C\", \"26\": \"C\", \"27\": \"C\", \"28\": \"C\", \"29\": \"C\",\n    \"30\": \"C\", \"31\": \"C\", \"32\": \"C\", \"33\": \"C\",\n\n    # Section D: Electricity, gas, steam and air conditioning supply\n    \"35\": \"D\",\n\n    # Section E: Water supply; sewerage, waste management and remediation\n    \"36\": \"E\", \"37\": \"E\", \"38\": \"E\", \"39\": \"E\",\n\n    # Section F: Construction\n    \"41\": \"F\", \"42\": \"F\", \"43\": \"F\",\n\n    # Section G: Wholesale and retail trade\n    \"45\": \"G\", \"46\": \"G\", \"47\": \"G\",\n\n    # Section H: Transportation and storage\n    \"49\": \"H\", \"50\": \"H\", \"51\": \"H\", \"52\": \"H\", \"53\": \"H\",\n\n    # Section I: Accommodation and food service activities\n    \"55\": \"I\", \"56\": \"I\",\n\n    # Section J: Information and communication\n    \"58\": \"J\", \"59\": \"J\", \"60\": \"J\", \"61\": \"J\", \"62\": \"J\", \"63\": \"J\",\n\n    # Section K: Financial and insurance activities\n    \"64\": \"K\", \"65\": \"K\", \"66\": \"K\",\n\n    # Section L: Real estate activities\n    \"68\": \"L\",\n\n    # Section M: Professional, scientific and technical activities\n    \"69\": \"M\", \"70\": \"M\", \"71\": \"M\", \"72\": \"M\", \"73\": \"M\", \"74\": \"M\", \"75\": \"M\",\n\n    # Section N: Administrative and support service activities\n    \"77\": \"N\", \"78\": \"N\", \"79\": \"N\", \"80\": \"N\", \"81\": \"N\", \"82\": \"N\",\n\n    # Section O: Public administration and defence\n    \"84\": \"O\",\n\n    # Section P: Education\n    \"85\": \"P\",\n\n    # Section Q: Human health and social work activities\n    \"86\": \"Q\", \"87\": \"Q\", \"88\": \"Q\",\n\n    # Section R: Arts, entertainment and recreation\n    \"90\": \"R\", \"91\": \"R\", \"92\": \"R\", \"93\": \"R\",\n\n    # Section S: Other service activities\n    \"94\": \"S\", \"95\": \"S\", \"96\": \"S\",\n\n    # Section T: Activities of households as employers\n    \"97\": \"T\", \"98\": \"T\",\n\n    # Section U: Activities of extraterritorial organizations\n    \"99\": \"U\"\n}\n\n\ndf['label'] = df['nace'].apply(lambda x: x[:2]).map(nace_2digit_to_section)\ndf['label'].value_counts()\n\nlabel\nG    122216\nM    103344\nI     93743\nL     92525\nF     61098\nS     60125\nH     50241\nN     48516\nQ     40265\nJ     32783\nC     28091\nR     26212\nK     25289\nP     25269\nA     22429\nD     12550\nE      1655\nO       161\nB       129\nU        11\nName: count, dtype: int64\n\n\nWe see that some sections, particularly O, B and U have very few training samples. Moreover there are no samples for section T. We are going to generate samples for these sections. To obtain the best results, it is important to generate samples at the most disaggregated level (class) and then aggregate them to the desired level. In this way we can leverage all the information contained in the explanatory notes.\n\n\n2.2 Explanatory notes\nWe will use the information in the explanatory notes of the statistical classification (NACE in this case) in order to provide the necessary information to the LLM to generate new samples. Explanatory notes contain a lot of high-quality information for each class of the statistical classifcation. Typically the information provided consists of: - Code of the class - Title of the class - Includes: a description of activities included in this class. - Also includes: a description of some activities which are also included in this class. - Not includes: a description of some activities which are not included in the class, but are related to activities which are included.\nIn order to use the information contained in the explanatory notes in an easy way it is convenient to prepare them in a machine-readable format, following the above categories. This can be done in formats like XML or JSON, or simply in an excel or csv file. In this case we have prepared an excel file with the information contained in the NAF notes. At Statistics Spain we provide an excel file with this information (https://www.ine.es/daco/daco42/clasificaciones/cnae25/notas_explicativas_CNAE_2025.xlsx)\n\nNAFnotes = pd.read_excel(\"NAFclasses.xlsx\", keep_default_na=False)\nNAFnotes.head()\n\n\n\n\n\n\n\n\nSection\nCode\nTitle\nIncludes\nAlso includes\nNot includes\n\n\n\n\n0\nA\n01.11Y\nCulture de céréales, à l'exception du riz, de...\nCette sous-classe comprend toutes les formes d...\n\nCette sous-classe ne comprend pas : - la cultu...\n\n\n1\nA\n01.12Y\nCulture du riz\nCette sous-classe comprend : - la culture du riz\n\n\n\n\n2\nA\n01.13Y\nCulture de légumes, de melons, de racines et ...\nCette sous-classe comprend : - la culture de l...\nCette sous-classe comprend également : - la cu...\nCette sous-classe ne comprend pas : - la cultu...\n\n\n3\nA\n01.14Y\nCulture de la canne à sucre\n\n\nCette sous-classe ne comprend pas : - la cultu...\n\n\n4\nA\n01.15Y\nCulture du tabac\nCette sous-classe comprend : - la culture de t...\nCette sous-classe comprend également : - la pr...\nCette sous-classe ne comprend pas : - la prépa...",
    "crumbs": [
      "Data augmentation"
    ]
  },
  {
    "objectID": "chapters/1_DataAugmentation/data_augmentation.html#generation-of-synthetic-data",
    "href": "chapters/1_DataAugmentation/data_augmentation.html#generation-of-synthetic-data",
    "title": "LLM-based synthetic data generation",
    "section": "3. Generation of synthetic data",
    "text": "3. Generation of synthetic data\nOur strategy is based on the paper ZeroGen (https://arxiv.org/abs/2202.07922). The idea is simple but extremely powerful: we leverage the generative abilities of LLMs to generate plausible samples (similar to real-world samples) for generating synthetic samples from the high-quality information contained in the explanatory notes.\nWe want to generate synthetic samples following two fundamental principles: - Faithfulness: the generated samples should be correctly coded and should be relevant to the problem at hand. - Diversity: the generated samples should display enough variety, both in style and content.\nIn this section, we will first explain how to setup an LLM API to make calls via Python, and then show how we can implement a simple data generation strategy using LLMs in an automated way. In order to generate faithful samples for each class, we need to provide the LLM with precise information, which we take from the class description in the explanatory notes. Even though in this tutorial we will develop classifiers at the section level, we will generate samples at the most disaggregated level. In general, even if we are interested in classifying at an aggregated level, it is useful to generate always the samples at the most disaggregated level, and later aggregate, in order to take full advantage of the explanatory notes and to get the maximum possible diversity of samples.\n\n3.1 Setting up the LLM API\nFirst, we show how to set up an LLM API. There are essentially two ways to use an LLM: locally or remotely. Both have advantages and disadvantages: - Local LLMs: uses an LLM which is directly run in our computer (or the organization servers). Using an LLM locally allow us to have complete control over the LLM and avoids cost and security concerns. However, our organization needs the adequate hardware (GPUs), especially for the larger models. We are limited to use only open-source models (excluding e.g. most of OpenAI models). Setting up the LLM for local use involves some extra steps before we can access it via an API. Fortunately, nowadays there is software which makes this step very easy, like Ollama or vLLM. - Remote LLMs: uses an LLM which runs in remote servers, typically in the premises of the provider of the model (OpenAI, Meta, Microsoft, …). The advantages are that they are easier to set up and use, allow us to run very large models without having to care about computing resources, and allow us to run closed-source models like most of OpenAI models. The disadvantages are the costs (each API call costs a quantity of money), security concerns, and less control over the models (they can update a model or discontinue it).\nIn this tutorial we will use for simplicity a remote provider, Groq (do not confuse with Grok, the family of xAI models), which has the advantage of allowing some free tokens and which is the provider we have used at Statistics Spain.\nGroq uses the OpenAI API to make the calls to the LLMs. Therefore, we must make sure we include the line: import openai in our code. The next step is to log in (or create an account) at the Groq website: https://groq.com/ , go to “API Keys” and click on “Create API key”. After introducing a name the key will be created. We must copy the key and put it in a python variable:\n\nAPI_KEY='&lt;INSERT_YOUR_API_KEY_HERE&gt;'\n\nNow to set up the client we execute the next chunk of code:\n\n# Preparing Groq API\nclient = openai.OpenAI(\n    api_key=API_KEY,\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\nWe now can use the models provided by Groq. To do it we use the next code, giving it a prompt:\n\n# Groq call\nresponse = client.responses.create(\n    model=\"llama-3.3-70b-versatile\", # model we want to use\n    input=\"Hi! How are you?\",        # prompt\n)\n\nIt returns us a Response object, which contains all the information of the LLM call. Here we will only be interested in the output of the model, which can be accessed as follows:\n\nresponse.output_text\n\n\"Hello. I'm just a computer program, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"\n\n\n\n\n3.2 Data generation from the explanatory notes\nWe use the explanatory notes of the statistical classification in order to generate new samples. Since the explanatory notes are a high-quality source, we expect that there are few mislabelled samples in the generated samples. Indeed, empirically it has been verified that the synthetic data generated in this way generally contains much less mistakes than real-world data.\nOur strategy for data generation will consist in constructing a generic prompt, which can be easily adapted to each class, that includes the title of the class and the relevant information from the explanatory notes, and then pass this prompt to an LLM which will generate the desired number of samples.\nIn order to obtain the best possible results it is important to construct the prompt carefully. While this is a highly empirical task, there are some guidelines which have proven to be universally useful (see, for instance, Section 3.1 of https://arxiv.org/abs/2406.15126v1). A well-constructed prompt should consist of the following parts: - Task specification: A description of the task the LLM must fulfill. A prologue to set the scene, such as “You are an expert in training data generation for classification models” often improves performance. - Conditions: The conditions and instructions the LLM must follow in the generation task. - In-context examples: Examples of the expected output. This has been observed to provide a huge performance improvement.\n\ndef generate_prompt(root, text, number):\n    prompt_process_notes = (\n                \"Vous êtes un expert en génération de données d'entraînement pour les modèles de classification. \"\n                 \"Votre tâche consiste à prendre une phrase racine d'une activité économique générale et à générer des phrases d'entraînement à partir d'exemples concrets inclus dans un texte. \"\n                 \"Le cas échéant, essayez d'introduire le mot 'autre' ou 'autre' ainsi que l'utilisation ou la destination du produit. \"\n                 \"Chaque fois qu'un objet est fabriqué, incluez le matériau à partir duquel il est fait. \"\n                 \"Évitez de produire des phrases complètement différentes ou changeant de contexte et ne retournez QUE des phrases sans introduction, commentaire, explication ou phrase supplémentaire. \"\n                 \"Évitez d'introduire vos résultats par des phrases telles que 'voici les phrases générées' ou similaires. \"\n                 \"Je veux utiliser votre résultat directement, donc je veux UNIQUEMENT les phrases. \"\n                 \"Voici un exemple : \"\n                 \"Racine : Production de produits de boulangerie et de confiserie. \"\n                 \"Texte : Cette classe comprend la fabrication de tous les produits de boulangerie tels que : pains, gâteaux, autres farines et pains destinés à l'alimentation animale. \"\n                 \"Sortie : \"\n                 \"FABRICATION DE MICHES DE PAIN. \"\n                 \"PRODUCTION DE PAIN POUR ANIMAUX. \"\n                 \"PRODUCTION DE GÂTEAUX. \"\n                 \"PRODUCTION D'AUTRES FARINES. \"\n                f\"Maintenant vous le faites avec la racine et le texte suivants, en générant {number} phrases: \"\n                f\"Racine: {root}. \"\n                f\"Texte: {text}.\"\n            )\n    return prompt_process_notes\n\nFirst we see how it works in one specific case, step by step. We will use subclass 25.11Y.\n\nprompt = generate_prompt(\"Fabrication de structures métalliques et de parties de structures\", \n                         \"\"\"Cette sous-classe comprend :\n                            - la fabrication de cadres métalliques ou d’ossatures pour la construction et de leurs éléments \n                            (tours, pylônes en treillis, armatures, ponts, éléments de ponts, etc.)\n                            - la fabrication de matériel d'échafaudage, de coffrage ou d'étayage\n                            - la fabrication de cadres métalliques pour équipements industriels (cadres pour hauts fourneaux, \n                            matériels de manutention, etc.)\n                            - la fabrication de constructions préfabriquées principalement en métaux :\n                            • baraques de chantier, éléments modulaires pour expositions, etc.\n                            \"\"\", \n                         10)\nprompt\n\n\"Vous êtes un expert en génération de données d'entraînement pour les modèles de classification. Votre tâche consiste à prendre une phrase racine d'une activité économique générale et à générer des phrases d'entraînement à partir d'exemples concrets inclus dans un texte. Le cas échéant, essayez d'introduire le mot 'autre' ou 'autre' ainsi que l'utilisation ou la destination du produit. Chaque fois qu'un objet est fabriqué, incluez le matériau à partir duquel il est fait. Évitez de produire des phrases complètement différentes ou changeant de contexte et ne retournez QUE des phrases sans introduction, commentaire, explication ou phrase supplémentaire. Évitez d'introduire vos résultats par des phrases telles que 'voici les phrases générées' ou similaires. Je veux utiliser votre résultat directement, donc je veux UNIQUEMENT les phrases. Voici un exemple : Racine : Production de produits de boulangerie et de confiserie. Texte : Cette classe comprend la fabrication de tous les produits de boulangerie tels que : pains, gâteaux, autres farines et pains destinés à l'alimentation animale. Sortie : FABRICATION DE MICHES DE PAIN. PRODUCTION DE PAIN POUR ANIMAUX. PRODUCTION DE GÂTEAUX. PRODUCTION D'AUTRES FARINES. Maintenant vous le faites avec la racine et le texte suivants, en générant 10 phrases: Racine: Fabrication de structures métalliques et de parties de structures. Texte: Cette sous-classe comprend :\\n                            - la fabrication de cadres métalliques ou d’ossatures pour la construction et de leurs éléments \\n                            (tours, pylônes en treillis, armatures, ponts, éléments de ponts, etc.)\\n                            - la fabrication de matériel d'échafaudage, de coffrage ou d'étayage\\n                            - la fabrication de cadres métalliques pour équipements industriels (cadres pour hauts fourneaux, \\n                            matériels de manutention, etc.)\\n                            - la fabrication de constructions préfabriquées principalement en métaux :\\n                            • baraques de chantier, éléments modulaires pour expositions, etc.\\n                            .\"\n\n\n\n# Groq call\nresponse = client.responses.create(\n    model=\"llama-3.3-70b-versatile\",\n    input=prompt,\n)\n\n\nresponse.output_text.split('\\n')\n\n['FABRICATION DE CADRES MÉTALLIQUES POUR CONSTRUCTION.',\n 'PRODUCTION DE PYLÔNES EN TREILLIS EN MÉTAL POUR APPUI DE STRUCTURES.',\n 'CONSTRUCTION DE PONTS MÉTALLIQUES DESTINÉS À LA CIRCULATION ROUDRIÈRE.',\n \"FABRICATION DE MATÉRIEL D'ÉCHAFAUDAGE EN MÉTAL POUR TRAVAUX DE CONSTRUCTION.\",\n 'PRODUCTION DE COFFRAGE MÉTALLIQUE POUR BÉTONNAGE.',\n 'CONSTRUCTION DE BARAQUES DE CHANTIER EN MÉTAL POUR HÉBERGEMENT DE TRAVAILLEURS.',\n 'FABRICATION DE CADRES MÉTALLIQUES POUR ÉQUIPEMENTS INDUSTRIELS TELS QUE HAUTS FOURNEAUX.',\n \"PRODUCTION D'ÉLÉMENTS MODULAIRES EN MÉTAL POUR EXPOSITIONS ET ÉVÉNEMENTS.\",\n \"FABRICATION DE STRUCTURES MÉTALLIQUES D'ÉTAYAGE POUR APPUI DE CONSTRUCTIONS.\",\n \"CONSTRUCTION DE TOURS MÉTALLIQUES DESTINÉES À D'AUTRES USAGE DANS L'INDUSTRIE.\"]\n\n\nNow we can prepare, following the same steps, a general function that generates samples for a given class in an automatic fashion:\n\ndef generate_samples(NAFclass, num_samples, notes = NAFnotes):\n    class_notes = NAFnotes[NAFnotes['Code']== NAFclass].iloc[0]\n    # 1. Generate the prompt\n    if class_notes['Includes']:\n        prompt = generate_prompt(class_notes['Title'], class_notes['Includes'], num_samples)\n    else: # If includes section is empty\n        prompt = generate_prompt(class_notes['Title'], class_notes['Title'], num_samples)\n    \n    # 2. Generate samples by calling Groq API\n    response = client.responses.create(\n                model=\"llama-3.3-70b-versatile\",\n                input=prompt,\n                )\n    samples = response.output_text.split('\\n')\n    return pd.DataFrame({'text':samples, 'class':NAFclass})\n\n\nNAFclass = '25.11Y'\ngenerate_samples(NAFclass, 10, notes = NAFnotes)\n\n\n\n\n\n\n\n\ntext\nclass\n\n\n\n\n0\nFABRICATION DE CADRES MÉTALLIQUES POUR CONSTRU...\n25.11Y\n\n\n1\nPRODUCTION DE TOURS ET PYLONES EN TREILLIS EN ...\n25.11Y\n\n\n2\nCONSTRUCTION D'ARMATURES MÉTALLIQUES POUR PONT.\n25.11Y\n\n\n3\nFABRICATION DE MATÉRIEL D'ÉCHAFAUDAGE EN MÉTAL.\n25.11Y\n\n\n4\nPRODUCTION D'ÉLÉMENTS DE PONT EN MÉTAL.\n25.11Y\n\n\n5\nFABRICATION DE CADRES MÉTALLIQUES POUR ÉQUIPEM...\n25.11Y\n\n\n6\nCONSTRUCTION DE BARAQUES DE CHANTIER PRINCIPAL...\n25.11Y\n\n\n7\nFABRICATION D'ÉLÉMENTS MODULAIRES POUR EXPOSIT...\n25.11Y\n\n\n8\nPRODUCTION DE COFFRAGE MÉTALLIQUE POUR CONSTRU...\n25.11Y\n\n\n9\nPRODUCTION D'AUTRES STRUCTURES MÉTALLIQUES POU...\n25.11Y\n\n\n\n\n\n\n\nFinally, we can generate a function for generating synthetic samples for a given section. The strategy is to generate some examples for each of the classes contained in the section using our previous function, and the put everything together.\n\ndef generate_samples_section(section, num_samples, notes = NAFnotes):\n    samples_df = pd.DataFrame()\n    classes = notes[notes['Section'] == section]['Code'] # Select classes of the given section from the notes\n    num_classes = len(classes) # Number of classes in the given section\n    \n    # For each class generate samples and concatenate with the samples already generated. \n    # We generate int(num_samples/num_classes)+1 samples for each class in the section,\n    # so in total we get approx num_samples for the section.\n    for NAFclass in classes:\n        samples_df = pd.concat([samples_df, generate_samples(NAFclass, int(num_samples/num_classes)+1, notes)]) \n        \n    samples_df['label']=section # Add the section as label\n    \n    return samples_df\n\n\nsamplesT = generate_samples_section('T', 50)\n\n\nsamplesT\n\n\n\n\n\n\n\n\ntext\nclass\nlabel\n\n\n\n\n0\nEMPLOI DE PERSONNEL DE NETTOYAGE.\n97.00Y\nT\n\n\n1\nEMPLOI DE CUISINIÈRES.\n97.00Y\nT\n\n\n2\nGESTION DU PERSONNEL DE MAISON.\n97.00Y\nT\n\n\n3\nEMPLOI DE PERSONNEL D'ENTRETIENT DOMESTIQUE.\n97.00Y\nT\n\n\n4\nEMPLOI DE GARDIENNES D'ENFANTS.\n97.00Y\nT\n\n\n5\nEMPLOI DE PERSONNEL DE SERVICE À DOMICILE.\n97.00Y\nT\n\n\n6\nRECOURS À DES CUISINIERES À DOMICILE.\n97.00Y\nT\n\n\n7\nEMPLOI DE PERSONNEL DE MAISON POUR L'ENTRETIEN...\n97.00Y\nT\n\n\n8\nEMPLOI DE PERSONNEL DE NETTOYAGE À DOMICILE.\n97.00Y\nT\n\n\n9\nEMPLOI DE PERSONNEL DE SOUTIEN À DOMICILE POUR...\n97.00Y\nT\n\n\n10\nEMPLOI DE PERSONNEL DE SERVICE DOMESTIQUE POUR...\n97.00Y\nT\n\n\n11\nEMPLOI D'AUTRE PERSONNEL DE SERVICE À DOMICILE.\n97.00Y\nT\n\n\n12\nEMPLOI DE PERSONNEL DE MAISON POUR LES TÂCHES ...\n97.00Y\nT\n\n\n13\nEMPLOI DE PERSONNEL DE NETTOYAGE POUR LES BURE...\n97.00Y\nT\n\n\n14\nGESTION DU PERSONNEL DOMESTIQUE POUR LES FAMIL...\n97.00Y\nT\n\n\n15\nEMPLOI DE PERSONNEL DE SERVICE DOMESTIQUE POUR...\n97.00Y\nT\n\n\n0\nPRODUCTION DE NOURRITURE PAR CHASSE.\n98.10Y\nT\n\n\n1\nCULTURE DE LÉGUMES POUR USAGE PROPRE.\n98.10Y\nT\n\n\n2\nÉLEVAGE DE POULETS POUR PRODUCTION DE VIANDE.\n98.10Y\nT\n\n\n3\nFABRICATION D'ABRIS EN BOIS POUR USAGE PROPRE.\n98.10Y\nT\n\n\n4\nCONFECTION DE VÊTEMENTS EN COTON POUR USAGE FA...\n98.10Y\nT\n\n\n5\nCUEILLETTE DE FRUITS SAUVAGES POUR CONSOMMATIO...\n98.10Y\nT\n\n\n6\nPRODUCTION DE LAIT CRU À PARTIR D'ÉLEVAGE DE V...\n98.10Y\nT\n\n\n7\nCULTURE DE CÉRÉALES POUR PRODUCTION DE FARINE.\n98.10Y\nT\n\n\n8\nÉLEVAGE DE LAPINS POUR PRODUCTION DE FOURRURE.\n98.10Y\nT\n\n\n9\nFABRICATION D'OBJETS EN OSIER POUR USAGE MÉNAG...\n98.10Y\nT\n\n\n10\nPRODUCTION DE MIEL À PARTIR D'ÉLEVAGE D'ABEILL...\n98.10Y\nT\n\n\n11\nCULTURE DE PLANTES MÉDICINALES POUR USAGE PROP...\n98.10Y\nT\n\n\n12\nCONFECTION DE MEUBLES EN BOIS POUR USAGE DOMES...\n98.10Y\nT\n\n\n13\nFABRICATION D'AUTRES PRODUITS ARTISANAUX POUR ...\n98.10Y\nT\n\n\n14\nCULTURE DE FLEURS ORNEMENTALES POUR DÉCORATION...\n98.10Y\nT\n\n\n15\nÉLEVAGE DE POISONS POUR PRODUCTION DE POISSON ...\n98.10Y\nT\n\n\n16\nFABRICATION DE TISSUS EN LIN POUR CONFECTION D...\n98.10Y\nT\n\n\n0\nPRODUCTION DE SERVICES DE SUBSISTANCE POUR USA...\n98.20Y\nT\n\n\n1\nPRODUCTION DE SERVICES DE CUISINE POUR SUBSIST...\n98.20Y\nT\n\n\n2\nPRODUCTION DE SERVICES D'ENSEIGNEMENT POUR MEM...\n98.20Y\nT\n\n\n3\nPRESTATION DE SOINS AUX MEMBRES DU MÉNAGE.\n98.20Y\nT\n\n\n4\nPRODUCTION D'AUTRES SERVICES POUR SUBSISTANCE ...\n98.20Y\nT\n\n\n5\nPRODUCTION DE SERVICES DE SUBSISTANCE POUR MEM...\n98.20Y\nT\n\n\n6\nACTIVITÉS DE PRODUCTION DE SERVICES DE SUBSIST...\n98.20Y\nT\n\n\n7\nPRODUCTION DE SERVICES POUR SUBSISTANCE DES ME...\n98.20Y\nT\n\n\n8\nPRODUCTION DE SERVICES DE CUISINE POUR MEMBRES...\n98.20Y\nT\n\n\n9\nPRODUCTION D'AUTRES SERVICES DE SUBSISTANCE PO...\n98.20Y\nT\n\n\n10\nPRODUCTION DE SERVICES D'ENSEIGNEMENT POUR SUB...\n98.20Y\nT\n\n\n11\nPRESTATION DE SOINS POUR MEMBRES DU MÉNAGE EN ...\n98.20Y\nT\n\n\n12\nPRODUCTION DE SERVICES POUR SUBSISTANCE PROPRE...\n98.20Y\nT\n\n\n13\nPRODUCTION DE SERVICES DE SUBSISTANCE POUR L'U...\n98.20Y\nT\n\n\n14\nACTIVITÉS DE PRODUCTION DE SERVICES POUR SUBSI...\n98.20Y\nT\n\n\n15\nPRODUCTION D'AUTRES SERVICES DE SUBSISTANCE PO...\n98.20Y\nT\n\n\n16\nPRODUCTION DE SERVICES DE SUBSISTANCE POUR L'U...\n98.20Y\nT\n\n\n\n\n\n\n\n\nsynthetic_df = pd.concat([generate_samples_section(section, 200) for section in ['O','B','U','T']])\n\n\nsynthetic_df\n\n\n\n\n\n\n\n\ntext\nclass\nlabel\n\n\n\n\n0\nADMINISTRATION DE L'EXÉCUTIF CENTRAL.\n84.11Y\nO\n\n\n1\nADMINISTRATION DE L'EXÉCUTIF RÉGIONAL.\n84.11Y\nO\n\n\n2\nADMINISTRATION DE L'EXÉCUTIF LOCAL.\n84.11Y\nO\n\n\n3\nACTIVITÉS DES MINISTÈRES AU NIVEAU NATIONAL.\n84.11Y\nO\n\n\n4\nACTIVITÉS DES MINISTÈRES AU NIVEAU RÉGIONAL.\n84.11Y\nO\n\n\n...\n...\n...\n...\n\n\n50\nPRODUCTION DE PRODUITS DE COMMUNICATION POUR L...\n98.20Y\nT\n\n\n51\nPRODUCTION DE PRODUITS INFORMATIQUES POUR LE M...\n98.20Y\nT\n\n\n52\nPRODUCTION DE PRODUITS DE RÉSEAUX POUR LA MAISON.\n98.20Y\nT\n\n\n53\nPRODUCTION DE PRODUITS POUR LA SÉCURITÉ INFORM...\n98.20Y\nT\n\n\n54\nPRODUCTION D'AUTRES PRODUITS POUR LE MÉNAGE.\n98.20Y\nT\n\n\n\n\n798 rows × 3 columns\n\n\n\n\nsynthetic_df = synthetic_df.drop(columns=['class'])\n\n\nsynthetic_df.to_csv(\"synthetic_df.csv\", index=False)\n\nThis strategy consists of taking the titles of the classes of the explanatory notes, remove the stopwords to get the keywords, use an LLM to generate variations or synonims of these keywords, and then obtain new samples by replacing one (or more) keywords by some of their synonims.\n\ndef tokenizer(text):\n        words = word_tokenize(text)\n        stop_words = set(stopwords.words('french'))\n        stop_words.update(punctuation)\n        words_list = [word for word in words if word not in stop_words]\n        return list(set(words_list))\n\n\ndef generate_prompt_dict(root, text):\n    prompt_make_dict = (\n        \"Je crée des phrases d'entraînement pour entraîner de manière supervisée un modèle de classification \"\n        \"des activités économiques sur la base d'un standard. \"\n        \"Je dispose d'un titre de classe et d'une description de chaque classe possible du standard. \"\n        \"J'ai besoin que tu me génères un dictionnaire de synonymes afin de constituer un jeu d'entraînement pour cette classe. \"\n        \"Une liste de mots et la description de la classe te seront fournies. \"\n        \"Je veux que tu me renvoies tous les exemples concrets ou synonymes des mots inclus dans la liste qui NE figurent PAS dans le texte. \"\n        \"De plus, ces exemples doivent avoir du sens dans le contexte de la classe afin de ne pas générer de bruit dans le modèle. \"\n        \"Si tu ne trouves aucun mot pour lequel tu peux donner des exemples, renvoie uniquement le mot NADA. \"\n        \"NE fais dans ta sortie aucune introduction comme 'voici les synonymes générés' ou similaires. \"\n        \"Je veux utiliser directement ta sortie, donc je veux UNIQUEMENT les réponses sous forme de dictionnaire en format Python. \"\n        \"Voici un exemple : \"\n        \"Liste de mots : [Élaboration, conserves, poissons]. \"\n        \"Texte : Cette classe comprend l’élaboration de conserves de poisson comme : melva, saumon ou sardine. \"\n        \"Sortie : \"\n        \"{'poissons': ['thon', 'maquereau', 'espadon', 'anchois'], \"\n        \"'élaboration': ['fabrication', 'manufacture', 'production']} \"\n        \"Maintenant, fais-le toi-même. \"\n        f\"Liste de mots : {root}. \"\n        f\"Texte : {text}\"\n    )\n    return prompt_make_dict\n\n\ndef generate_simple_sentences(text, word_dict, key_list):\n        final_list = [text]\n        for key in key_list:\n            text_list = [text.replace(key, word) for word in word_dict[key]]\n            final_list.extend(text_list)\n        return final_list\n\n\nroot = \"Extraction de gaz naturel\"\ntext = \"\"\"Cette sous-classe comprend :\n        - la production d’hydrocarbures gazeux bruts (gaz naturel)\n        - l’extraction de condensats\n        - la décantation et la séparation de fractions d’hydrocarbures liquides\n        - la désulfuration du gaz\n        - l’extraction de méthane provenant des mines de charbon\n        \"\"\"\n\n\nkey_list = tokenizer(root)\nkey_list\n\n['naturel', 'gaz', 'Extraction']\n\n\n\nprompt = generate_prompt_dict(key_list, text)\nprompt\n\n\"Je crée des phrases d'entraînement pour entraîner de manière supervisée un modèle de classification des activités économiques sur la base d'un standard. Je dispose d'un titre de classe et d'une description de chaque classe possible du standard. J'ai besoin que tu me génères un dictionnaire de synonymes afin de constituer un jeu d'entraînement pour cette classe. Une liste de mots et la description de la classe te seront fournies. Je veux que tu me renvoies tous les exemples concrets ou synonymes des mots inclus dans la liste qui NE figurent PAS dans le texte. De plus, ces exemples doivent avoir du sens dans le contexte de la classe afin de ne pas générer de bruit dans le modèle. Si tu ne trouves aucun mot pour lequel tu peux donner des exemples, renvoie uniquement le mot NADA. NE fais dans ta sortie aucune introduction comme 'voici les synonymes générés' ou similaires. Je veux utiliser directement ta sortie, donc je veux UNIQUEMENT les réponses sous forme de dictionnaire en format Python. Voici un exemple : Liste de mots : [Élaboration, conserves, poissons]. Texte : Cette classe comprend l’élaboration de conserves de poisson comme : melva, saumon ou sardine. Sortie : {'poissons': ['thon', 'maquereau', 'espadon', 'anchois'], 'élaboration': ['fabrication', 'manufacture', 'production']} Maintenant, fais-le toi-même. Liste de mots : ['naturel', 'gaz', 'Extraction']. Texte : Cette sous-classe comprend :\\n        - la production d’hydrocarbures gazeux bruts (gaz naturel)\\n        - l’extraction de condensats\\n        - la décantation et la séparation de fractions d’hydrocarbures liquides\\n        - la désulfuration du gaz\\n        - l’extraction de méthane provenant des mines de charbon\\n        \"\n\n\n\n# Groq call\nresponse = client.responses.create(\n    model=\"llama-3.3-70b-versatile\",\n    input=prompt,\n)\n\n\nword_dict = eval(response.output_text)\nword_dict\n\n{'naturel': ['biologique', 'organique', 'écologique'],\n 'gaz': ['méthane', 'butane', 'propane', 'éthane'],\n 'Extraction': ['récupération', 'exploitation', 'production', 'capture']}\n\n\n\ngenerate_simple_sentences(root, word_dict, key_list)\n\n['Extraction de gaz naturel',\n 'Extraction de gaz biologique',\n 'Extraction de gaz organique',\n 'Extraction de gaz écologique',\n 'Extraction de méthane naturel',\n 'Extraction de butane naturel',\n 'Extraction de propane naturel',\n 'Extraction de éthane naturel',\n 'récupération de gaz naturel',\n 'exploitation de gaz naturel',\n 'production de gaz naturel',\n 'capture de gaz naturel']\n\n\n\n\n3.3 Some ideas for further improvement\nWe have seen an example of a basic generation strategy. However, there is plenty of research to be done in this direction, and there are lots of experiments one can perform. Here we just give some ideas or suggestions that one could try: - Prompt engineering. Try different prompt structures or variations. Try giving as examples real-world samples from the class we want to generate samples instead of using the same generic example for all classes. - Improve diversity of samples by asking an LLM to generate it in different styles (longer, shorter, with ortographical errors, etc). - Improve further the diversity by asking several distinct LLMs to produce samples. - Use not just the “Includes” section of the explanatory notes, but also the “Also includes” and the “Not included” section. - Exploit the hierarchical structure of the classification in the prompt, giving not only information about the class but also the higher groups. - Data curation: improve the quality of the generated samples by including a process of filtering (by humans or by another LLM)",
    "crumbs": [
      "Data augmentation"
    ]
  },
  {
    "objectID": "index.html#part-1-synthetic-data-generation-to-improve-the-training-data-set",
    "href": "index.html#part-1-synthetic-data-generation-to-improve-the-training-data-set",
    "title": "Tutorial for WP10 text classifiction",
    "section": "Part 1: Synthetic data generation to improve the training data set",
    "text": "Part 1: Synthetic data generation to improve the training data set\nIn practice, training data sets that include statistical classification codes are often imbalanced, with certain categories significantly underrepresented. Therefore, some categories do not have enough data for the model to properly learn. One approach for mitigating this issue is data augmentation. In this context, we explore how LLMs together with the official explanatory notes of the classification can be leveraged to generate synthetic data points.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#part-2-fine-tuning-a-transformer-pipeline-for-text-classification",
    "href": "index.html#part-2-fine-tuning-a-transformer-pipeline-for-text-classification",
    "title": "Tutorial for WP10 text classifiction",
    "section": "Part 2: Fine-tuning a transformer pipeline for text classification",
    "text": "Part 2: Fine-tuning a transformer pipeline for text classification\nIn this part, we will learn how to fine-tune transformer models from Hugging Face for text classification tasks. Using pre-labeled datasets, we will demonstrate how to fine-tune a pretrained model and evaluate its performance on a test set. We illustrate the pipeline on an open data set that links company descriptions with corresponding NACE codes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#part-3-setting-up-a-simple-rag-pipeline-for-automatic-text-classification",
    "href": "index.html#part-3-setting-up-a-simple-rag-pipeline-for-automatic-text-classification",
    "title": "Tutorial for WP10 text classifiction",
    "section": "Part 3: Setting up a simple RAG pipeline for automatic text classification",
    "text": "Part 3: Setting up a simple RAG pipeline for automatic text classification\nWe will explore an alternative approach to text classification using a simple Retrieval-Augmented Generation (RAG) pipeline. We will construct a knowledge base using official NACE category descriptions. Applying semantic search, we then select the k-closest classes for a given user query. Finally we will use an LLM to generate classification predictions.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#dependancy-management",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#dependancy-management",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Dependancy management",
    "text": "Dependancy management\nHere we import all dependancies that we will need to use\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n    pipeline,\n    set_seed\n)\nfrom datasets import load_dataset\nfrom datasets import Dataset, ClassLabel, DatasetDict\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, top_k_accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport random",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#configuration-variables-and-parameters",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#configuration-variables-and-parameters",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Configuration variables and parameters",
    "text": "Configuration variables and parameters\nHere, we can set some parameters with arbitrary value for importing and training.\n\n\nModel Settings\n\n\n\n\n\n\n\n\n\nParameter\nType\nExample Value\nDescription\n\n\n\n\nmodel_id\nstr\n'bert-base-multilingual-uncased'\nThe Hugging Face model ID to load from the hub. Here, a multilingual BERT model is used for supporting multiple languages.\n\n\nmax_seq_len\nint\n256\nThe maximum number of tokens in an input sequence. Longer sequences will be truncated.\n\n\n\n\n\n\nOutput Settings\n\n\n\n\n\n\n\n\n\nParameter\nType\nExample Value\nDescription\n\n\n\n\noutput_dir\nstr\n'saved_models/bert-base-multilingual-uncased'\nDirectory where the trained model, tokenizer, and training logs will be saved.\n\n\n\n\n\n\nTraining Hyperparameters\n\n\n\n\n\n\n\n\n\nParameter\nType\nExample Value\nDescription\n\n\n\n\nepochs\nint\n4\nNumber of training epochs. One epoch means going through the full dataset once.\n\n\nlearn_rate\nfloat\n5e-5\nInitial learning rate for the optimizer (AdamW by default).\n\n\nscheduler\nstr\n'linear'\nLearning rate scheduler type. 'linear' gradually decreases the LR after a warmup period.\n\n\ntrain_bs\nint\n16\nBatch size for training steps.\n\n\neval_bs\nint\n32\nBatch size for evaluation steps.\n\n\nga_steps\nint\n2\nGradient accumulation steps. Allows you to simulate a larger batch size without increasing GPU memory usage.\n\n\ndecay\nfloat\n0.01\nWeight decay to prevent overfitting by penalizing large weights.\n\n\nwarmup\nfloat\n0.1\nFraction of total training steps used for learning rate warmup.\n\n\n\n\n\n\nEvaluation & Logging\n\n\n\n\n\n\n\n\n\nParameter\nType\nExample Value\nDescription\n\n\n\n\neval_strategy\nstr\n'epoch'\nWhen to run evaluation. 'epoch' means after each epoch.\n\n\nlogging_strategy\nstr\n'epoch'\nWhen to log metrics. 'epoch' means at the end of each epoch.\n\n\nsave_strategy\nstr\n'no'\nWhen to save model checkpoints. 'no' means only final save at the end of training.\n\n\nlog_level\nstr\n'warning'\nLogging verbosity. Options include 'debug', 'info', 'warning', 'error'.\n\n\nreport_to\nlist\n[]\nList of reporting integrations (\"wandb\", \"tensorboard\", etc.). Empty means no external reporting.\n\n\n# log_steps\nint\n(commented out)\nIf enabled, logs training metrics every log_steps steps.\n\n\n\n\n\n\nPrecision & Model Loading\n\n\n\n\n\n\n\n\n\nParameter\nType\nExample Value\nDescription\n\n\n\n\nfp16\nbool\nFalse\nWhether to use 16-bit floating-point precision (mixed precision) for faster and memory-efficient training.\n\n\nload_best\nbool\nFalse\nWhether to load the best checkpoint after training based on evaluation metrics.\n\n\n\n\n\nNotes\n\nGradient Accumulation (ga_steps): With train_bs = 16 and ga_steps = 2, the effective batch size is 16 * 2 = 32.\nWarmup (warmup): If you have 1000 total steps, warmup=0.1 means the first 100 steps will gradually ramp up the learning rate.\nMixed Precision (fp16): Useful on GPUs with Tensor Cores (e.g., NVIDIA RTX series) to speed up training and reduce memory usage.\n\n\n\nmodel_id        : str   = f'bert-base-multilingual-uncased'\nmax_seq_len     : int   = 256\n\noutput_dir      : str   = f'saved_models/{model_id}'\nepochs          : int   = 4\nlearn_rate      : float = 5e-5\nscheduler       : str   = 'linear'\ntrain_bs        : int   = 16\neval_bs         : int   = 32\nga_steps        : int   = 2\ndecay           : float = 0.01\nwarmup          : float = 0.1\neval_strategy   : str   = 'epoch'\nlogging_strategy: str   = 'epoch'\nsave_strategy   : str   = 'no'\nfp16            : bool  = False\nload_best       : bool  = False\nreport_to       : list  = []\nlog_level       : str   = 'warning'\n\nSEED            : int   = 42\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nset_seed(SEED)",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#convert-dataframe-to-hugging-face-dataset",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#convert-dataframe-to-hugging-face-dataset",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Convert DataFrame to Hugging Face Dataset",
    "text": "Convert DataFrame to Hugging Face Dataset\nTransforming a Pandas DataFrame into a Hugging Face Dataset makes it directly compatible with the Trainer API. This enables efficient tokenization, easy dataset splitting, and optimized batch processing.",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#convert-string-labels-to-integers-using-labelencoder",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#convert-string-labels-to-integers-using-labelencoder",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Convert string labels to integers using LabelEncoder",
    "text": "Convert string labels to integers using LabelEncoder\nMachine learning models require labels as numeric IDs instead of text. Encoding labels ensures they are in a format the model can use.",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#keep-id2label-and-label2id",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#keep-id2label-and-label2id",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Keep id2label and label2id",
    "text": "Keep id2label and label2id\nThese mappings connect numeric label IDs with their human-readable names. id2label converts predictions into class names for interpretability, while label2id ensures correct label-to-ID conversion during training. Storing them in the model configuration makes inference outputs understandable.",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#use-classlabel-and-stratified-split",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#use-classlabel-and-stratified-split",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Use ClassLabel and Stratified Split",
    "text": "Use ClassLabel and Stratified Split\nClassLabel preserves both the numeric ID and the original label name inside the dataset, improving readability and compatibility. A stratified split ensures that the proportion of each class is maintained between the training and validation sets, leading to more reliable evaluation results.\n\ndf = pd.read_csv(\n    \"data/raw/nace_train.csv\", # TODO: change to augmented dataset\n    index_col=0\n)\n\n\ndata = DatasetDict({\n    'train': Dataset.from_pandas(df)\n})\n\n\ndata['train'][0]\n\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(data['train']['label'])\n\n# Generate mappings\nid2label = {i: str(label) for i, label in enumerate(label_encoder.classes_)}\nlabel2id = {label: i for i, label in id2label.items()}\n\nclass_label = ClassLabel(names=label_encoder.classes_.tolist())\n\n\ndata = data.map(lambda x: {'label': label_encoder.transform(x['label'])}, batched=True)\n# Map your dataset to use the ClassLabel feature for stratification\ndata = data.cast_column('label', class_label)\n\n\ndata = data['train'].train_test_split(test_size=0.05, seed=SEED, stratify_by_column=\"label\")\ndata[\"validation\"] = data.pop(\"test\")",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#loading-a-pretrained-model",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#loading-a-pretrained-model",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Loading a Pretrained Model",
    "text": "Loading a Pretrained Model\nAutoModelForSequenceClassification.from_pretrained(...) downloads (or loads from cache) a transformer model designed for text classification.\n\nmodel_id: Identifies the model on the Hugging Face Hub (e.g., \"bert-base-multilingual-uncased\").\n\nnum_labels: Sets the number of output classes for the classification task.\n\nid2label / label2id: Provide mappings between numeric label IDs and human-readable labels, stored in the model configuration so predictions can be interpreted later.\n\n.to(device): Moves the model’s weights to the chosen hardware (CPU or GPU) for faster computation.\n\nInteraction with Hugging Face Hub\nWhen called for the first time with a given model_id, Hugging Face will: 1. Check the local cache (default: ~/.cache/huggingface/transformers or path from HF_HOME env variable). 2. If not found locally, download the model weights and configuration from the Hugging Face Hub. 3. Save them in the cache for future runs, avoiding repeated downloads.",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#loading-the-tokenizer",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#loading-the-tokenizer",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Loading the Tokenizer",
    "text": "Loading the Tokenizer\nAutoTokenizer.from_pretrained(model_id) loads the tokenizer that matches the chosen model.\n\nRetrieves vocabulary, tokenization rules, and preprocessing steps needed to convert raw text into token IDs.\nEnsures tokenization is consistent with the model’s training setup.\nUses the same cache mechanism as the model loader: checks local cache, downloads from the Hub if necessary, then stores locally.",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#remarks",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#remarks",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "Remarks",
    "text": "Remarks\n\nThe model and tokenizer must match — both are tied to the same model_id to ensure correct input formatting.\nUsing from_pretrained makes it easy to reuse pretrained weights and tokenizers without manual file handling.\nThe cache system speeds up experimentation, as once a model/tokenizer is downloaded, subsequent runs use the local copy instantly.\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_id,\n    num_labels=len(id2label), \n    id2label=id2label, \n    label2id=label2id,\n).to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#compute_metrics-function",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#compute_metrics-function",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "4.1. compute_metrics Function",
    "text": "4.1. compute_metrics Function\nThis function calculates multiple evaluation metrics for a classification model.\nIt is designed to be passed to Hugging Face’s Trainer, which automatically calls it during evaluation.\n\n\nInputs\n\neval_pred: A tuple (logits, labels) provided by the Trainer.\n\nlogits: Model outputs before activation (shape: [batch_size, num_classes]).\nlabels: Ground truth class IDs.\n\n\n\n\n\nSteps\n\nUnpack predictions and labels\n\nExtracts logits and labels from the tuple.\n\nConvert logits to predicted class IDs\n\nUses np.argmax(logits, axis=-1) to choose the class with the highest logit score for each sample.\n\nDetermine the number of classes\n\nReads num_classes from logits.shape[1].\nCreates class_labels as a range from 0 to num_classes - 1 to ensure all possible classes are considered in top-k metrics.\n\nCompute metrics\n\nAccuracy: Percentage of correct predictions.\nF1 Macro: F1 score averaged across all classes equally.\nPrecision Macro: Average precision across all classes, weighted equally.\nRecall Macro: Average recall across all classes, weighted equally.\nTop-1 Accuracy: Accuracy when considering only the single most likely prediction.\nTop-2 Accuracy: Accuracy when considering the two most likely predictions (checks if the correct class is in the top-2 predicted classes).\n\nzero_division=0 ensures no errors if a class is missing in predictions or labels.\nReturn results\n\nReturns a dictionary with all computed metrics.\nHugging Face’s Trainer logs these values and uses them for evaluation reports.\n\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    \n    num_classes = logits.shape[1]\n    class_labels = np.arange(num_classes)  # Ensure all classes are covered\n    \n    accuracy = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n    top_1_acc = top_k_accuracy_score(labels, logits, k=1, labels=class_labels)\n    top_2_acc = top_k_accuracy_score(labels, logits, k=2, labels=class_labels)\n\n    return {\n        'accuracy': accuracy,\n        'f1_macro': f1,\n        'precision_macro': precision,\n        'recall_macro': recall,\n        'top_1_accuracy': top_1_acc,\n        'top_2_accuracy': top_2_acc,\n    }\n\nNow, we define the training arguments and the trainer class.",
    "crumbs": [
      "Transformers for text classification"
    ]
  },
  {
    "objectID": "chapters/2_FineTune/fine-tuning-bert-example.html#datacollatorwithpadding",
    "href": "chapters/2_FineTune/fine-tuning-bert-example.html#datacollatorwithpadding",
    "title": "Fine-Tuning a Transformers Model Guide",
    "section": "4.2. DataCollatorWithPadding",
    "text": "4.2. DataCollatorWithPadding\nThe DataCollatorWithPadding is a utility from Hugging Face’s transformers library that handles dynamic padding for batches during training and evaluation.\n\nHow it works\n\nLooks at all sequences in the current batch.\nFinds the longest sequence in that batch.\nPads all other sequences to match that length.\nUses the tokenizer to add the correct padding tokens and attention masks.\n\n\n\nWhy use it\n\nMemory efficient – avoids padding all sequences to a fixed max_seq_len.\nFaster training – smaller average sequence length per batch means fewer computations.\nCleaner code – no need to pre-pad the dataset manually.\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=epochs,\n    learning_rate=learn_rate,\n    lr_scheduler_type=scheduler,\n    per_device_train_batch_size=train_bs,\n    per_device_eval_batch_size=eval_bs,\n    gradient_accumulation_steps=ga_steps,\n    warmup_ratio=warmup,\n    weight_decay=decay,\n    logging_dir='./logs',\n    # logging_steps=log_steps,\n    logging_strategy=logging_strategy,\n    eval_strategy=eval_strategy,\n    save_strategy=save_strategy,\n    fp16=fp16,\n    load_best_model_at_end=load_best,\n    report_to=report_to,\n    log_level=log_level,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_data['train'],\n    eval_dataset=tokenized_data['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator\n)\n\nFinally, we can start training the model.\n\n%%time\ntrainer.train()",
    "crumbs": [
      "Transformers for text classification"
    ]
  }
]