{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67b4541",
   "metadata": {},
   "source": [
    "<a href=\"\" target=\"_blank\" rel=\"noopener\" data-original-href=\"\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Launch_this_turorial_with_GPU_supported_JupyterLab-blue?logo=jupyter&amp;logoColor=white\" alt=\"Onyxia\"></a>\n",
    "\n",
    "::: {.content-hidden}\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python-gpu?name=jupyter-python&version=2.3.19&s3=region-79669f20&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2FAIML4OS%2FWP10_tutorial_text_classification%2Frefs%2Fheads%2Fdev%2Ffinetune%2Fsspcloud%2F3_RAG%2Finit-trainees-RAG.sh»&autoLaunch=true\" target=\"_blank\" rel=\"noopener\" data-original-href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?name=jupyter-python&version=2.3.19&s3=region-79669f20&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2FAIML4OS%2FWP10_tutorial_text_classification%2Frefs%2Fheads%2Fdev%2Ffinetune%2Fsspcloud%2F3_RAG%2Finit-trainees-RAG.sh»&autoLaunch=true\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Launch_this_turorial_with_GPU_supported_JupyterLab-blue?logo=jupyter&amp;logoColor=white\" alt=\"Onyxia\"></a>\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd06d3-4c26-4ba5-8b17-89e55fd48182",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Transformers Model Guide\n",
    "\n",
    "In this tutorial, we’ll build a text classifier by fine-tuning a pretrained BERT model from Hugging Face’s Transformers library.\n",
    "We’ll start from a very practical point: you already have a labeled dataset stored in a CSV file, where each row contains a piece of text and its corresponding label.\n",
    "By the end, you’ll know how to:\n",
    "\n",
    "1. Load a CSV dataset and convert it into the Hugging Face Datasets format\n",
    "\n",
    "2. Load a pretrained model and tokenizer from the Hugging Face Hub\n",
    "\n",
    "3. Tokenize text using the model’s tokenizer\n",
    "\n",
    "4. Fine-tune the model with the Transformers Trainer API\n",
    "\n",
    "5. Save and load model in local file system\n",
    "\n",
    "6. Evaluate the model and run predictions on test dataset with transformers pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f78e4-67e8-4fc0-b558-38f2410e5906",
   "metadata": {},
   "source": [
    "## Dependancy management\n",
    "\n",
    "Here we import all dependancies that we will need to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247af92c-1254-40be-8fb6-848450d821f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, ClassLabel, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, top_k_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b612e0-b27a-4198-99fb-946b01c8d077",
   "metadata": {},
   "source": [
    "## Configuration variables and parameters\n",
    "\n",
    "Here, we can set some parameters with arbitrary value for importing and training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Settings**\n",
    "| Parameter | Type | Example Value | Description |\n",
    "|-----------|------|---------------|-------------|\n",
    "| `model_id` | `str` | `'bert-base-multilingual-uncased'` | The Hugging Face model ID to load from the hub. Here, a multilingual BERT model is used for supporting multiple languages. |\n",
    "| `max_seq_len` | `int` | `256` | The maximum number of tokens in an input sequence. Longer sequences will be truncated. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Output Settings**\n",
    "| Parameter | Type | Example Value | Description |\n",
    "|-----------|------|---------------|-------------|\n",
    "| `output_dir` | `str` | `'saved_models/bert-base-multilingual-uncased'` | Directory where the trained model, tokenizer, and training logs will be saved. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Hyperparameters**\n",
    "| Parameter | Type | Example Value | Description |\n",
    "|-----------|------|---------------|-------------|\n",
    "| `epochs` | `int` | `4` | Number of training epochs. One epoch means going through the full dataset once. |\n",
    "| `learn_rate` | `float` | `5e-5` | Initial learning rate for the optimizer (AdamW by default). |\n",
    "| `scheduler` | `str` | `'linear'` | Learning rate scheduler type. `'linear'` gradually decreases the LR after a warmup period. |\n",
    "| `train_bs` | `int` | `16` | Batch size for training steps. |\n",
    "| `eval_bs` | `int` | `32` | Batch size for evaluation steps. |\n",
    "| `ga_steps` | `int` | `2` | Gradient accumulation steps. Allows you to simulate a larger batch size without increasing GPU memory usage. |\n",
    "| `decay` | `float` | `0.01` | Weight decay to prevent overfitting by penalizing large weights. |\n",
    "| `warmup` | `float` | `0.1` | Fraction of total training steps used for learning rate warmup. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation & Logging**\n",
    "| Parameter | Type | Example Value | Description |\n",
    "|-----------|------|---------------|-------------|\n",
    "| `eval_strategy` | `str` | `'epoch'` | When to run evaluation. `'epoch'` means after each epoch. |\n",
    "| `logging_strategy` | `str` | `'epoch'` | When to log metrics. `'epoch'` means at the end of each epoch. |\n",
    "| `save_strategy` | `str` | `'no'` | When to save model checkpoints. `'no'` means only final save at the end of training. |\n",
    "| `log_level` | `str` | `'warning'` | Logging verbosity. Options include `'debug'`, `'info'`, `'warning'`, `'error'`. |\n",
    "| `report_to` | `list` | `[]` | List of reporting integrations (`\"wandb\"`, `\"tensorboard\"`, etc.). Empty means no external reporting. |\n",
    "| `# log_steps` | `int` | *(commented out)* | If enabled, logs training metrics every `log_steps` steps. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Precision & Model Loading**\n",
    "| Parameter | Type | Example Value | Description |\n",
    "|-----------|------|---------------|-------------|\n",
    "| `fp16` | `bool` | `False` | Whether to use 16-bit floating-point precision (mixed precision) for faster and memory-efficient training. |\n",
    "| `load_best` | `bool` | `False` | Whether to load the best checkpoint after training based on evaluation metrics. |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Notes**\n",
    "- **Gradient Accumulation (`ga_steps`)**: With `train_bs = 16` and `ga_steps = 2`, the *effective batch size* is `16 * 2 = 32`.\n",
    "- **Warmup (`warmup`)**: If you have 1000 total steps, `warmup=0.1` means the first 100 steps will gradually ramp up the learning rate.\n",
    "- **Mixed Precision (`fp16`)**: Useful on GPUs with Tensor Cores (e.g., NVIDIA RTX series) to speed up training and reduce memory usage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae37359-4749-441e-90c6-945d6ef32247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id        : str   = f'bert-base-multilingual-uncased'\n",
    "max_seq_len     : int   = 256\n",
    "\n",
    "output_dir      : str   = f'saved_models/{model_id}'\n",
    "epochs          : int   = 4\n",
    "learn_rate      : float = 5e-5\n",
    "scheduler       : str   = 'linear'\n",
    "train_bs        : int   = 16\n",
    "eval_bs         : int   = 32\n",
    "ga_steps        : int   = 2\n",
    "decay           : float = 0.01\n",
    "warmup          : float = 0.1\n",
    "eval_strategy   : str   = 'epoch'\n",
    "logging_strategy: str   = 'epoch'\n",
    "save_strategy   : str   = 'no'\n",
    "fp16            : bool  = False\n",
    "load_best       : bool  = False\n",
    "report_to       : list  = []\n",
    "log_level       : str   = 'warning'\n",
    "\n",
    "SEED            : int   = 42\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392175d-afcd-4a35-bf93-3a2161ba485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d19f3-428b-4cac-9cf0-5c1e0215f559",
   "metadata": {},
   "source": [
    "# 1. Load a CSV dataset and convert it into the Hugging Face Datasets format\n",
    "\n",
    "\n",
    "## Convert DataFrame to Hugging Face Dataset\n",
    "Transforming a Pandas DataFrame into a Hugging Face `Dataset` makes it directly compatible with the `Trainer` API. This enables efficient tokenization, easy dataset splitting, and optimized batch processing.\n",
    "\n",
    "## Convert string labels to integers using LabelEncoder  \n",
    "Machine learning models require labels as numeric IDs instead of text. Encoding labels ensures they are in a format the model can use.\n",
    "\n",
    "## Keep `id2label` and `label2id`  \n",
    "These mappings connect numeric label IDs with their human-readable names. `id2label` converts predictions into class names for interpretability, while `label2id` ensures correct label-to-ID conversion during training. Storing them in the model configuration makes inference outputs understandable.\n",
    "\n",
    "## Use ClassLabel and Stratified Split  \n",
    "`ClassLabel` preserves both the numeric ID and the original label name inside the dataset, improving readability and compatibility. A stratified split ensures that the proportion of each class is maintained between the training and validation sets, leading to more reliable evaluation results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49301db0-046f-42af-b2d6-426f33f206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"data/raw/nace_train.csv\", # TODO: change to augmented dataset\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb4c91-a93d-476b-9517-f1884d3360f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3891988-c059-42a1-957e-277b50f26a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6282b25-1e24-474a-9978-8aae7ed4ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['train']['label'])\n",
    "\n",
    "# Generate mappings\n",
    "id2label = {i: str(label) for i, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "\n",
    "class_label = ClassLabel(names=label_encoder.classes_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e3d45-0201-4b6d-b797-a93d2f9015cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x: {'label': label_encoder.transform(x['label'])}, batched=True)\n",
    "# Map your dataset to use the ClassLabel feature for stratification\n",
    "data = data.cast_column('label', class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e812b5b-48e0-4ad2-a2d2-f45fe91f973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['train'].train_test_split(test_size=0.05, seed=SEED, stratify_by_column=\"label\")\n",
    "data[\"validation\"] = data.pop(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deedae4-8ead-472b-9593-319265cc3fdb",
   "metadata": {},
   "source": [
    "# 2. Load a pretrained model and tokenizer from the Hugging Face Hub\n",
    "Load the model and tokenizer from huggingface. If the model is gated or private, you need to set an environment variable called \"HF_TOKEN\" that contans your huggingface token.\n",
    "\n",
    "## **Loading a Pretrained Model**  \n",
    "`AutoModelForSequenceClassification.from_pretrained(...)` downloads (or loads from cache) a transformer model designed for text classification.\n",
    "\n",
    "- **`model_id`**: Identifies the model on the Hugging Face Hub (e.g., `\"bert-base-multilingual-uncased\"`).  \n",
    "- **`num_labels`**: Sets the number of output classes for the classification task.  \n",
    "- **`id2label`** / **`label2id`**: Provide mappings between numeric label IDs and human-readable labels, stored in the model configuration so predictions can be interpreted later.  \n",
    "- **`.to(device)`**: Moves the model’s weights to the chosen hardware (CPU or GPU) for faster computation.\n",
    "\n",
    "**Interaction with Hugging Face Hub**  \n",
    "When called for the first time with a given `model_id`, Hugging Face will:\n",
    "1. **Check the local cache** (default: `~/.cache/huggingface/transformers` or path from `HF_HOME` env variable).\n",
    "2. If not found locally, **download the model weights and configuration** from the Hugging Face Hub.\n",
    "3. Save them in the cache for future runs, avoiding repeated downloads.\n",
    "\n",
    "---\n",
    "\n",
    "## **Loading the Tokenizer**  \n",
    "`AutoTokenizer.from_pretrained(model_id)` loads the tokenizer that matches the chosen model.\n",
    "\n",
    "- Retrieves **vocabulary, tokenization rules, and preprocessing steps** needed to convert raw text into token IDs.\n",
    "- Ensures **tokenization is consistent** with the model’s training setup.\n",
    "- Uses the same **cache mechanism** as the model loader: checks local cache, downloads from the Hub if necessary, then stores locally.\n",
    "\n",
    "---\n",
    "\n",
    "## **Remarks**  \n",
    "- The **model** and **tokenizer** must match — both are tied to the same `model_id` to ensure correct input formatting.\n",
    "- Using `from_pretrained` makes it easy to reuse pretrained weights and tokenizers without manual file handling.\n",
    "- The **cache system** speeds up experimentation, as once a model/tokenizer is downloaded, subsequent runs use the local copy instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadea798-7cb1-40c8-a94e-72ef7a7c2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(id2label), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b03b9-0ec9-4e9b-bf95-ed3a369751cf",
   "metadata": {},
   "source": [
    "# 3. Tokenize text using the model’s tokenizer\n",
    "Now we tokenize and pad the data using the pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54703c8b-2b8b-42cb-b3dc-4d6023b47dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=max_seq_len)\n",
    "\n",
    "tokenized_data = data.map(\n",
    "    tokenize,\n",
    "    batched=True\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395638a-9cd8-4538-92ac-815c6fe83226",
   "metadata": {},
   "source": [
    "# 4. Fine-tune the model with the Transformers Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796a353-8f5c-4c7a-93ff-7ce3480df8e9",
   "metadata": {},
   "source": [
    "## 4.1. `compute_metrics` Function\n",
    "\n",
    "This function calculates multiple evaluation metrics for a classification model.  \n",
    "It is designed to be passed to Hugging Face’s `Trainer`, which automatically calls it during evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Inputs**\n",
    "- **`eval_pred`**: A tuple `(logits, labels)` provided by the Trainer.\n",
    "  - `logits`: Model outputs before activation (shape: `[batch_size, num_classes]`).\n",
    "  - `labels`: Ground truth class IDs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "1. **Unpack predictions and labels**\n",
    "   - Extracts `logits` and `labels` from the tuple.\n",
    "\n",
    "2. **Convert logits to predicted class IDs**\n",
    "   - Uses `np.argmax(logits, axis=-1)` to choose the class with the highest logit score for each sample.\n",
    "\n",
    "3. **Determine the number of classes**\n",
    "   - Reads `num_classes` from `logits.shape[1]`.\n",
    "   - Creates `class_labels` as a range from `0` to `num_classes - 1` to ensure all possible classes are considered in top-k metrics.\n",
    "\n",
    "4. **Compute metrics**\n",
    "   - **Accuracy**: Percentage of correct predictions.\n",
    "   - **F1 Macro**: F1 score averaged across all classes equally.\n",
    "   - **Precision Macro**: Average precision across all classes, weighted equally.\n",
    "   - **Recall Macro**: Average recall across all classes, weighted equally.\n",
    "   - **Top-1 Accuracy**: Accuracy when considering only the single most likely prediction.\n",
    "   - **Top-2 Accuracy**: Accuracy when considering the two most likely predictions (checks if the correct class is in the top-2 predicted classes).\n",
    "\n",
    "   `zero_division=0` ensures no errors if a class is missing in predictions or labels.\n",
    "\n",
    "5. **Return results**\n",
    "   - Returns a dictionary with all computed metrics.  \n",
    "     Hugging Face’s `Trainer` logs these values and uses them for evaluation reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae0d1c-c956-4b44-a377-11e2304b33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    num_classes = logits.shape[1]\n",
    "    class_labels = np.arange(num_classes)  # Ensure all classes are covered\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
    "    top_1_acc = top_k_accuracy_score(labels, logits, k=1, labels=class_labels)\n",
    "    top_2_acc = top_k_accuracy_score(labels, logits, k=2, labels=class_labels)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1,\n",
    "        'precision_macro': precision,\n",
    "        'recall_macro': recall,\n",
    "        'top_1_accuracy': top_1_acc,\n",
    "        'top_2_accuracy': top_2_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e47c6-da84-45e1-a363-a474f66756f4",
   "metadata": {},
   "source": [
    "Now, we define the training arguments and the trainer class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260aa8e7-6d96-4c9b-a67a-ff7b33932285",
   "metadata": {},
   "source": [
    "## 4.2. DataCollatorWithPadding\n",
    "\n",
    "The `DataCollatorWithPadding` is a utility from Hugging Face's `transformers` library that handles **dynamic padding** for batches during training and evaluation.\n",
    "\n",
    "### How it works\n",
    "- Looks at all sequences in the current batch.\n",
    "- Finds the **longest sequence** in that batch.\n",
    "- Pads all other sequences to match that length.\n",
    "- Uses the tokenizer to add the correct padding tokens and attention masks.\n",
    "\n",
    "### Why use it\n",
    "- **Memory efficient** – avoids padding all sequences to a fixed `max_seq_len`.\n",
    "- **Faster training** – smaller average sequence length per batch means fewer computations.\n",
    "- **Cleaner code** – no need to pre-pad the dataset manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d17b9-215b-49a8-8888-b958a6f6908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=learn_rate,\n",
    "    lr_scheduler_type=scheduler,\n",
    "    per_device_train_batch_size=train_bs,\n",
    "    per_device_eval_batch_size=eval_bs,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    warmup_ratio=warmup,\n",
    "    weight_decay=decay,\n",
    "    logging_dir='./logs',\n",
    "    # logging_steps=log_steps,\n",
    "    logging_strategy=logging_strategy,\n",
    "    eval_strategy=eval_strategy,\n",
    "    save_strategy=save_strategy,\n",
    "    fp16=fp16,\n",
    "    load_best_model_at_end=load_best,\n",
    "    report_to=report_to,\n",
    "    log_level=log_level,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10e4d6-5eb4-4eba-8396-4c6ea335d9ab",
   "metadata": {},
   "source": [
    "Finally, we can start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f52b50c-79cd-4b8a-8dc1-7a50912fd00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2140' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2140/2140 01:55, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Top 1 Accuracy</th>\n",
       "      <th>Top 2 Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.319500</td>\n",
       "      <td>0.748003</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.687252</td>\n",
       "      <td>0.695802</td>\n",
       "      <td>0.684681</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.884444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>0.637802</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.791815</td>\n",
       "      <td>0.819281</td>\n",
       "      <td>0.793008</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.914444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.625531</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.816379</td>\n",
       "      <td>0.848717</td>\n",
       "      <td>0.807242</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.915556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.620195</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.818658</td>\n",
       "      <td>0.845393</td>\n",
       "      <td>0.814407</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.925556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 262 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2140, training_loss=0.6376569730099116, metrics={'train_runtime': 115.426, 'train_samples_per_second': 592.587, 'train_steps_per_second': 18.54, 'total_flos': 1458577288617840.0, 'train_loss': 0.6376569730099116, 'epoch': 4.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4064aa",
   "metadata": {},
   "source": [
    "# 5. Save and load model in local file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085aff0-77ae-4317-95b8-2f9eb420a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_save_path = 'models/localsave/bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf83449-429d-4bd9-aaea-52c769bc9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save it in local path\n",
    "# model.save_pretrained(local_save_path)\n",
    "# tokenizer.save_pretrained(local_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239a70c-ba15-4f98-b4ea-144913dd2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    local_save_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_save_path,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7346e9a-4a64-41eb-8c00-575d4804debb",
   "metadata": {},
   "source": [
    "# 6. Evaluate the model and run predictions on test dataset with transformers pipeline\n",
    "Now, we can evaluate the model on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d3765-0a4b-4808-ba6a-2617cb26f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task='text-classification',\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174fc13-31d8-4783-aed9-17a04ef987b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/raw/nace_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa3a09-26a9-446a-bfd4-ab7ce7fb3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b523dae-a479-4922-bc63-2bdca8efe4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test['label'].tolist()\n",
    "X_test = df_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090e2af-e962-47ef-8010-96bc8a223325",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = pipe(X_test)\n",
    "result_topk = pipe(X_test, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd305e-d28f-4772-a723-58d27a829ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [_['label'] for _ in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70ad51-89d4-4358-8a8b-1c0abd93d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898974a8-da20-4fef-b2e1-d6532de4b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance on test set \\n')\n",
    "print(f'Accuracy score  : {accuracy:.3f}')\n",
    "print(f'F1 score        : {f1:.3f}')\n",
    "print(f'precision score : {precision:.3f}')\n",
    "print(f'recall score    : {recall:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0dbd0-7c69-4caf-b2b7-113a0975c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability matrix\n",
    "num_samples = len(result_topk)\n",
    "num_classes = len(label2id)\n",
    "y_pred_proba = np.zeros((num_samples, num_classes))\n",
    "\n",
    "for i, sample in enumerate(result_topk):\n",
    "    for pred in sample:\n",
    "        class_idx = label2id[pred['label']]\n",
    "        y_pred_proba[i][class_idx] = pred['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc254999-0d58-4151-a25a-017a7af0bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1 = top_k_accuracy_score(y_test, y_pred_proba, k=1, labels=list(label2id.keys()))\n",
    "top2 = top_k_accuracy_score(y_test, y_pred_proba, k=2, labels=list(label2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d38557-e071-4253-9886-7cb573d20dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top 1 accuracy  : {top1:.3f}')\n",
    "print(f'Top 2 accuracy  : {top2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b53b38-d0fb-4d88-93e6-52bfaf581ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
