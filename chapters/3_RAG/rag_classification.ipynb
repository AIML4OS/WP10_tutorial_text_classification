{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-pytorch-gpu?name=WP10-tutorial-RAG&version=2.3.19&s3=region-79669f20&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2FAIML4OS%2FWP10_tutorial_text_classification%2Frefs%2Fheads%2Fmain%2Fsspcloud%2F3_RAG%2Finit-trainees-RAG.sh»&autoLaunch=true\" target=\"_blank\" rel=\"noopener\" data-original-href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-pytorch-gpu?name=WP10-tutorial-RAG&version=2.3.19&s3=region-79669f20&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2FAIML4OS%2FWP10_tutorial_text_classification%2Frefs%2Fheads%2Fmain%2Fsspcloud%2F3_RAG%2Finit-trainees-RAG.sh»&autoLaunch=true\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Launch_this_turorial_with_GPU_supported_JupyterLab-blue?logo=jupyter&amp;logoColor=white\" alt=\"Onyxia\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJBBMvco2ESl"
      },
      "source": [
        "# Chapter 5. **RAG Pipeline for Automatic Text Classification**\n",
        "In this chapter, we will explore an alternative approach to text classification building a simple retrieval-augmented generation (RAG) system. This chapter will cover:\n",
        "* Building a **knowledge base** using official NACE category descriptions.\n",
        "* **Retrieving** the `top_k` relevant categories based on a **natural language query**.\n",
        "* Using a **large language model** (LLM) to make the final classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![RAG System Scheme](resources/rag_scheme.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpcAvfZp1_VZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy0vaDlGKqlE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muH2Tfk33kFC"
      },
      "source": [
        "## Phase 1 - Knowledge Base Creation\n",
        "The first step in any RAG system is to build a **knowledge base**.\n",
        "> In the context of retrieval-augmented generation, a knowledge base is a structured **collection of external information** (e.g., documents, descriptions...) that can be **searched and retrieved from**. This is usually employed to ground LLM responses into factual information instead on relying solely on the model's internal knowledge, or to augment the model with knowledge that was not available in the training set.\n",
        "\n",
        "In order to build a knowledge base we need to:\n",
        "1. Define what documents to store and how to structure them.\n",
        "2. Embed the selected texts using a sentence-level embedding model (`SentenceTransformer`).\n",
        "3. Store the embeddings in memory or in a vector database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi6XyWpp5_wq"
      },
      "source": [
        "### Defining and structuring documents\n",
        "In our case, the knowledge base will be comprised by the NACE codes at the division level (2-digit). Therefore, the first step is to load the file containing all NACE codes and pre-process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_H5fchE445b"
      },
      "outputs": [],
      "source": [
        "nace_df = pd.read_excel(\"data/NACE_Rev2.1_Structure_Explanatory_Notes_EN.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mfoJeam-LiM"
      },
      "source": [
        "Now, we can isolate the NACE divisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YmmeZlm-QZ4"
      },
      "outputs": [],
      "source": [
        "nace_division_df = nace_df[nace_df[\"ID\"].str.len() == 2].reset_index(drop=True)\n",
        "\n",
        "nace_division_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8u4M56x-jwP"
      },
      "source": [
        "As we can see, the dataset has multiple textual columns, like titles and descriptions (*Includes*, etc.).\n",
        "> In general, **the more context** we give to the embedding model **the better**, as long as the final text to embed doesn't get too long.\n",
        "\n",
        "A simple way to exploit the textual information present in the NACE dataset, is to include different textual features into a **structured template**, which we can call **descriptor**. In this setting, each NACE division will be associated to one descriptor, made of its title and further information present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO550K2N_k4J"
      },
      "outputs": [],
      "source": [
        "codes = nace_division_df[\"ID\"].tolist()\n",
        "titles = nace_division_df[\"NAME\"].tolist()\n",
        "descriptions = nace_division_df[\"Includes\"].tolist()\n",
        "\n",
        "descriptor_template = \"{title}.\\n{description}\"\n",
        "\n",
        "descriptors = []\n",
        "for title, description in zip(titles, descriptions):\n",
        "    descriptors.append(\n",
        "        descriptor_template.format(title=title.upper(), description=description)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrtl_XmkAiWg"
      },
      "source": [
        "Let's print one descriptor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1C65w4LAOf3"
      },
      "outputs": [],
      "source": [
        "print(descriptors[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSiOF9CuAq3D"
      },
      "source": [
        "These descriptors will be the actual text that will be embedded to create the knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdl0y6TBBQAf"
      },
      "source": [
        "### Embed the descriptors\n",
        "Now that we built a descriptor for each of the 80+ NACE divisions, we need to embed them using a **sentence-level embedding model**. There are plenty of such models on Hugging Face. We can load them using the `SentenceTransformer` class from the `sentence_transformers` library.\n",
        "\n",
        "For this example, we will use the lightweight **multilingual** `paraphrase-multilingual-MiniLM-L12-v2` model from the S-BERT family. Since we need to match descriptions in French with NACE codes in English, it is important that our model can handle multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWXSZFmgBoCi"
      },
      "outputs": [],
      "source": [
        "MODEL_ID: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "model = SentenceTransformer(MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y18Ade-BBzGV"
      },
      "source": [
        "After loading the model, we can embed the descriptors. Each descriptor will be embedded into a **384-dimensional embedding vector**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTh3sYo1B25E"
      },
      "outputs": [],
      "source": [
        "embeddings = model.encode(descriptors, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjjfUWKlCLdv"
      },
      "source": [
        "Since we only have 87 NACE divisions (and therefore 87 vectors), for the purpose of this tutorial we can keep the embeddings in memory without relying on vector databases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnRvhkaEKoR6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP02kmnP57bk"
      },
      "source": [
        "## Phase 2 - Searching the Knowledge Base\n",
        "This phase consists in developing a method to search the knowledge base via semantic similarity.\n",
        "> When searching a knowledge base, a natural language query is embedded **via the same model used to create the knowledge base** and a **distance metric** is computed to evaluate the **similarity** between the query and the documents in the knowledge base.\n",
        "\n",
        "The most popular distance metric used to retrieve elements from a knowledge base is **cosine similarity**, defined as:\n",
        "$$\\cos (\\theta)=\\frac{\\mathbf{A}\\cdot \\mathbf{B}}{||\\mathbf{A}||||\\mathbf{B}||}$$\n",
        "where $\\theta$ is the angle between the embedding vectors $\\mathbf{A}$ and $\\mathbf{B}$.\n",
        "\n",
        "Now we can finally define a function that searches the knowledge base given a natural language query and retrieves the `top_k` most similar results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqQIgiBf5-AO"
      },
      "outputs": [],
      "source": [
        "def search_base(\n",
        "    query: str,\n",
        "    base: torch.Tensor,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    top_k: int = 5\n",
        "):\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = torch.nn.functional.cosine_similarity(base, query_embedding)\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    return top_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PP-z82SFeaV"
      },
      "source": [
        "Let's test the function with a sample query. We get both `values` (similarity scores) and `indices` as a result. In order to extract the **corresponding NACE divisions**, we can use the indices to filter the NACE dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_lV6cc1Fh8w"
      },
      "outputs": [],
      "source": [
        "sample_query = \"Attività di estrazione mineraria.\"\n",
        "\n",
        "top_results = search_base(sample_query, embeddings, model, top_k=5)\n",
        "\n",
        "for i, sim in zip(top_results.indices, top_results.values):\n",
        "    code = nace_division_df.iloc[int(i)][\"ID\"]\n",
        "    title = nace_division_df.iloc[int(i)][\"NAME\"]\n",
        "    print(f\"{code}: {title}\\nSimilarity: {sim:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdru8ec1K75b"
      },
      "source": [
        "In theory, we could stop here and just pick the first result as our classification guess. However, a more robust approach involves an LLM judge to pick between the candidates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCeHI69uKtMR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQkL9_Qk6P_y"
      },
      "source": [
        "## Phase 3 - Final LLM Classification\n",
        "This final stage comprises a generative large language model to pick an option between the retrieved candidate codes. First, we load the LLM via the Hugging Face `pipeline` wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3dAYNZE6UKw"
      },
      "outputs": [],
      "source": [
        "LLM_ID: str = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=LLM_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP_WR5odi2xc"
      },
      "source": [
        "### System Prompt and Instructions\n",
        "We need to define a **system prompt**, i.e., textual guidelines that will help the LLM follow our instructions, and the actual **prompt template** to parse the LLM input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4YmFYb_iRw-"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT: str = \"\"\"You are an economic activity classifier.\n",
        "\n",
        "Based on a description, return ONLY the most fitting candidate without any additional text.\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE: str = \"\"\"DESCRIPTION: \"{query}\"\n",
        "\n",
        "CANDIDATES:\\n{candidates}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWfqU4Jflluz"
      },
      "source": [
        "### Parsing the Input\n",
        "We need to extract the **list of candidates** using semantic-based similarity and parse it to make it understandable for the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkmFhfm5jHmg"
      },
      "outputs": [],
      "source": [
        "sample_query = \"Mining activities\"\n",
        "\n",
        "top_results = search_base(sample_query, embeddings, model, top_k=5)\n",
        "\n",
        "candidates_list = []\n",
        "for i, sim in zip(top_results.indices, top_results.values):\n",
        "    code = nace_division_df.iloc[int(i)][\"ID\"]\n",
        "    title = nace_division_df.iloc[int(i)][\"NAME\"]\n",
        "    candidates_list.append(f\"{code} - {title}\")\n",
        "\n",
        "candidates = \"\\n\".join(candidates_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4MPx6rZjo32"
      },
      "source": [
        "Let's print the parsed prompt to visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-39L1FYjtBh"
      },
      "outputs": [],
      "source": [
        "parsed_prompt = PROMPT_TEMPLATE.format(query=sample_query, candidates=candidates)\n",
        "\n",
        "print(parsed_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_7FfZ5riDrc"
      },
      "source": [
        "### Full RAG System\n",
        "Now, we can test the **full RAG system** to classify the economic activity given a natural language query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhT_cGUfgARy"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": parsed_prompt},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=32,\n",
        "    pad_token_id=pipe.tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm_out = outputs[0][\"generated_text\"][-1][\"content\"]\n",
        "print(llm_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk-M0B_IpIan"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-tBO9nzmH0Q"
      },
      "source": [
        "## Interactive Demo\n",
        "Finally, we can create a small **interactive demo** for our RAG system. First, we need to create a `rag` function to perform the whole pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adc9kyPsmMIS"
      },
      "outputs": [],
      "source": [
        "def rag(\n",
        "    query: str,\n",
        "    embeddings: torch.Tensor,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    llm: pipeline,\n",
        "    system_prompt: str,\n",
        "    prompt_template: str,\n",
        "    nace_df: pd.DataFrame,\n",
        "    top_k: int = 5\n",
        "):\n",
        "\n",
        "    top_results = search_base(query, embeddings, embedding_model, top_k=5)\n",
        "\n",
        "    candidates_list = []\n",
        "    for i, sim in zip(top_results.indices, top_results.values):\n",
        "        code = nace_df.iloc[int(i)][\"ID\"]\n",
        "        title = nace_df.iloc[int(i)][\"NAME\"]\n",
        "        candidates_list.append(f\"{code} - {title}\")\n",
        "\n",
        "    candidates = \"\\n\".join(candidates_list)\n",
        "\n",
        "    parsed_prompt = PROMPT_TEMPLATE.format(query=query, candidates=candidates)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": parsed_prompt},\n",
        "    ]\n",
        "\n",
        "    outputs = pipe(\n",
        "        messages,\n",
        "        max_new_tokens=16,\n",
        "        pad_token_id=pipe.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"][-1][\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORdWQMjrpDcZ"
      },
      "source": [
        "Now we can run the demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxnEUUuPm5qD"
      },
      "outputs": [],
      "source": [
        "query = input(\"Entry a query ('EXIT' to stop): \")\n",
        "\n",
        "while query != \"EXIT\":\n",
        "    output = rag(query, embeddings, model, pipe, SYSTEM_PROMPT, PROMPT_TEMPLATE, nace_division_df)\n",
        "    print(f\"RESULT: {output}\")\n",
        "    print(\"\")\n",
        "    query = input(\"Entry a query ('EXIT' to stop): \")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
